{
  "batch_mode": "truncate_episodes",
  "env": "mpe_simple_adversary",
  "evaluation_interval": 50,
  "framework": "torch",
  "lr": 0.0005,
  "model": {
    "custom_model": "ToMModel",
    "custom_model_config": {
      "activation": "relu",
      "agent_name_ls": [
        "adversary_0",
        "agent_0",
        "agent_1"
      ],
      "algo_args": {
        "batch_episode": 128,
        "batch_mode": "truncate_episodes",
        "clip_param": 0.3,
        "entropy_coeff": 0.01,
        "kl_coeff": 0.2,
        "lambda": 1.0,
        "lr": 0.0005,
        "num_sgd_iter": 10,
        "use_gae": true,
        "vf_clip_param": 20.0,
        "vf_loss_coeff": 1.0
      },
      "algorithm": "tommappo",
      "alpha": 0.5,
      "belief_dim": 1,
      "beta": 0.25,
      "checkpoint_end": true,
      "checkpoint_freq": 100,
      "env": "mpe",
      "env_args": {
        "N": 2,
        "continuous_actions": false,
        "map_name": "simple_adversary",
        "max_cycles": 25
      },
      "episode_limit": 25,
      "evaluation_interval": 50,
      "fcnet_activation": "relu",
      "force_coop": false,
      "framework": "torch",
      "gamma": 0.25,
      "global_state_flag": false,
      "local_dir": "",
      "local_mode": false,
      "mask_flag": false,
      "model_arch_args": {
        "fc_layer": 2,
        "out_dim_fc_0": 128,
        "out_dim_fc_1": 8
      },
      "num_agents": 3,
      "num_cpus_per_worker": 1,
      "num_gpus": 1,
      "num_gpus_per_worker": 0,
      "num_workers": 12,
      "opp_action_in_cc": true,
      "policy_mapping_info": {
        "simple_adversary": {
          "all_agents_one_policy": false,
          "description": "one team attack, one team survive",
          "one_agent_one_policy": true,
          "team_prefix": [
            "adversary_",
            "agent_"
          ]
        },
        "simple_crypto": {
          "all_agents_one_policy": false,
          "description": "two team cooperate, one team attack",
          "one_agent_one_policy": true,
          "team_prefix": [
            "eve_",
            "bob_",
            "alice_"
          ]
        },
        "simple_push": {
          "all_agents_one_policy": false,
          "description": "one team target on landmark, one team attack",
          "one_agent_one_policy": true,
          "team_prefix": [
            "adversary_",
            "agent_"
          ]
        },
        "simple_reference": {
          "all_agents_one_policy": true,
          "description": "one team cooperate",
          "one_agent_one_policy": true,
          "team_prefix": [
            "agent_"
          ]
        },
        "simple_speaker_listener": {
          "all_agents_one_policy": true,
          "description": "two team cooperate",
          "one_agent_one_policy": true,
          "team_prefix": [
            "speaker_",
            "listener_"
          ]
        },
        "simple_spread": {
          "all_agents_one_policy": true,
          "description": "one team cooperate",
          "one_agent_one_policy": true,
          "team_prefix": [
            "agent_"
          ]
        },
        "simple_tag": {
          "all_agents_one_policy": false,
          "description": "one team attack, one team survive",
          "one_agent_one_policy": true,
          "team_prefix": [
            "adversary_",
            "agent_"
          ]
        },
        "simple_world_comm": {
          "all_agents_one_policy": false,
          "description": "two team cooperate and attack, one team survive",
          "one_agent_one_policy": true,
          "team_prefix": [
            "adversary_",
            "leadadversary_",
            "agent_"
          ]
        }
      },
      "res_out_dim": 8,
      "restore_path": {
        "model_path": "",
        "params_path": ""
      },
      "seed": 321,
      "share_policy": "group",
      "space_act": "Discrete(5)",
      "space_obs": "Dict(obs:Box(-100.0, 100.0, (10,), float32))",
      "stop_iters": 9999999,
      "stop_reward": 999999,
      "stop_timesteps": 2000000
    },
    "max_seq_len": 25
  },
  "multiagent": {
    "policies": {
      "policy_adversary_": [
        null,
        "Dict(obs:Box(-100.0, 100.0, (10,), float32))",
        "Discrete(5)",
        {}
      ],
      "policy_agent_": [
        null,
        "Dict(obs:Box(-100.0, 100.0, (10,), float32))",
        "Discrete(5)",
        {}
      ]
    },
    "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x75eb77b91e50>"
  },
  "num_gpus": 1,
  "num_gpus_per_worker": 0,
  "num_workers": 12,
  "seed": 321,
  "simple_optimizer": false,
  "train_batch_size": 3200
}